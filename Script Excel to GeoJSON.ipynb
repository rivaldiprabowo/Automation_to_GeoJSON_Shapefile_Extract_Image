{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automation Excel to GeoJSON and Shapefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Application to Export Excel into GeoJson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Function Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString, MultiPoint\n",
    "import numpy as np\n",
    "\n",
    "def install_requirements(): #Install required packages if they are not already installed.\n",
    "    required = {\n",
    "        'pandas': '1.0.0',\n",
    "        'openpyxl': '3.0.0',\n",
    "        'geopandas': '0.9.0',\n",
    "        'shapely': '1.7.0',}\n",
    "    \n",
    "    installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}\n",
    "    \n",
    "    # Determine what needs to be installed\n",
    "    missing = []\n",
    "    update = []\n",
    "    \n",
    "    for package, min_version in required.items():\n",
    "        if package not in installed:\n",
    "            missing.append(package)\n",
    "        elif pkg_resources.parse_version(installed[package]) < pkg_resources.parse_version(min_version):\n",
    "            update.append(package)\n",
    "    \n",
    "    if missing or update: # If packages need to be installed or updated\n",
    "        print(\"Some required packages are missing or need to be updated.\")\n",
    "        print(f\"Missing: {', '.join(missing) if missing else 'None'}\")\n",
    "        print(f\"Need update: {', '.join(update) if update else 'None'}\")\n",
    "        \n",
    "        try:  # Install missing packages\n",
    "            if missing:\n",
    "                print(f\"Installing missing packages: {', '.join(missing)}\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + missing)\n",
    "\n",
    "            if update: # Update packages that need updating\n",
    "                print(f\"Updating packages: {', '.join(update)}\")\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\"] + update)\n",
    "                \n",
    "            print(\"All required packages have been installed/updated successfully!\")\n",
    "            \n",
    "            # Re-import the modules to make them available\n",
    "            if 'pandas' in missing or 'pandas' in update:\n",
    "                global pd\n",
    "                import pandas as pd\n",
    "            if 'openpyxl' in missing or 'openpyxl' in update:\n",
    "                global load_workbook\n",
    "                from openpyxl import load_workbook\n",
    "            if 'geopandas' in missing or 'geopandas' in update:\n",
    "                global gpd\n",
    "                import geopandas as gpd\n",
    "            if 'shapely' in missing or 'shapely.geometry' in update:\n",
    "                global Point, LineString\n",
    "                from shapely.geometry import Point, LineString\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to install required packages: {str(e)}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "def unique_column_names(columns): #Ensure column names are unique by appending suffix.\n",
    "    seen = {}\n",
    "    new_columns = []\n",
    "    for col in columns:\n",
    "        if col is None:\n",
    "            col = \"Unnamed\"\n",
    "        \n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            new_columns.append(f\"{col}_{seen[col]}\")\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "            new_columns.append(col)\n",
    "    return new_columns\n",
    "\n",
    "def clean_column_names(columns): #Standardize column names by capitalizing each word properly.\n",
    "    cleaned_columns = []\n",
    "    seen = {}\n",
    "    for col in columns:\n",
    "        if col is None:\n",
    "            col = \"Unnamed\"\n",
    "        else:\n",
    "            col = str(col).strip()\n",
    "            \n",
    "        if \"rekap\" in col.lower(): # Remove columns with \"Rekap\" in the name\n",
    "            continue\n",
    "\n",
    "        words = col.split() # Fix for duplicated words (like \"No No\", \"Detail Lokasi Detail Lokasi\")\n",
    "        if len(words) >= 2:\n",
    "            half_length = len(words) // 2 # Check for repeated word patterns\n",
    "            if words[:half_length] == words[half_length:] and len(words) % 2 == 0:\n",
    "                col = \" \".join(words[:half_length]) # If the first half matches the second half, only use the first half\n",
    "        \n",
    "        col = \" \".join(word.capitalize() for word in col.split())\n",
    "        \n",
    "        # Handle duplicates\n",
    "        if col in seen:\n",
    "            seen[col] += 1\n",
    "            col = f\"{col} {seen[col]}\"\n",
    "        else:\n",
    "            seen[col] = 0\n",
    "        \n",
    "        cleaned_columns.append(col)\n",
    "    return cleaned_columns\n",
    "\n",
    "def find_coordinate_columns(df, prefix, column_type): #Find coordinate columns with various naming patterns returns column name\n",
    "    \n",
    "    patterns = []\n",
    "    \n",
    "    if prefix == 'start':\n",
    "        if column_type == 'lat':\n",
    "            patterns = ['awal latitude','awal lat','koordinat latitude','koordinat lat','latitude awal','lat awal','latitude 1','lat 1']\n",
    "        else:\n",
    "            patterns = ['awal longitude','awal lon','koordinat longitude','koordinat lon','longitude awal', 'lon awal','longitude 1','lon 1' ]\n",
    "    else:\n",
    "        if column_type == 'lat':\n",
    "            patterns = ['akhir latitude','akhir lat','latitude akhir','lat akhir','latitude 2','lat 2']\n",
    "        else:\n",
    "            patterns = ['akhir longitude','akhir lon','longitude akhir','lon akhir','longitude 2','lon 2']\n",
    "    \n",
    "    # If we're looking for start coordinates, and just a single coordinate exists (no start/end distinction)\n",
    "    if prefix == 'start' and column_type == 'lat':\n",
    "        patterns.extend(['latitude', 'lat'])\n",
    "    elif prefix == 'start' and column_type == 'lon':\n",
    "        patterns.extend(['longitude', 'lon'])\n",
    "    \n",
    "    # Search for column containing any of the patterns\n",
    "    for pattern in patterns:\n",
    "        for col in df.columns:\n",
    "            if col is not None and isinstance(col, str) and pattern in col.lower():\n",
    "                return col\n",
    "    \n",
    "    return None\n",
    "\n",
    "def fix_coordinates(row, lat_col, lon_col): #Fix latitude and longitude values that may be in the wrong format.\n",
    "    # Defensive check to make sure columns exist in the row\n",
    "    if lat_col not in row or lon_col not in row:\n",
    "        return pd.Series([pd.NA, pd.NA])\n",
    "        \n",
    "    lat, lon = row[lat_col], row[lon_col]\n",
    "    original_lat, original_lon = lat, lon \n",
    "    \n",
    "    # Handle string values with commas (e.g., \"-69,694,951\" or \"1,065,663,308\")\n",
    "    if isinstance(lat, str):\n",
    "        try:\n",
    "            lat = float(lat.replace(',', ''))\n",
    "        except (ValueError, AttributeError):\n",
    "            lat = pd.NA\n",
    "    \n",
    "    if isinstance(lon, str):\n",
    "        try:\n",
    "            lon = float(lon.replace(',', ''))\n",
    "        except (ValueError, AttributeError):\n",
    "            lon = pd.NA\n",
    "    \n",
    "    # First attempt to detect the scale by number of digits\n",
    "    if pd.notna(lat):\n",
    "        lat_abs = abs(lat)\n",
    "        # For values like -6448977 (which should be around -6.4 degrees)\n",
    "        if 1_000_000 < lat_abs < 10_000_000 and str(int(lat_abs)).startswith(('6', '7', '8', '9')):\n",
    "            lat = lat / 1_000_000\n",
    "        elif lat_abs > 90:\n",
    "            if lat_abs > 10_000_000:  # Very large values\n",
    "                lat = lat / 10_000_000\n",
    "            elif lat_abs > 1_000_000:  # Large values (common for Indonesia coords)\n",
    "                lat = lat / 1_000_000\n",
    "            elif lat_abs > 90_000:\n",
    "                lat = lat / 1_000\n",
    "    \n",
    "    if pd.notna(lon):\n",
    "        lon_abs = abs(lon)\n",
    "        if 100_000_000 < lon_abs < 1_500_000_000:\n",
    "            lon = lon / 10_000_000\n",
    "        elif lon_abs > 180:\n",
    "            if lon_abs > 10_000_000:\n",
    "                lon = lon / 10_000_000\n",
    "            elif lon_abs > 1_000_000:\n",
    "                lon = lon / 1_000_000\n",
    "            elif lon_abs > 180_000:\n",
    "                lon = lon / 1_000\n",
    "    \n",
    "    return pd.Series([lat, lon])\n",
    "\n",
    "def parse_coordinate(coord_value): # Parse and clean coordinate values that may be stored in unwanted formats\n",
    "\n",
    "    if pd.isna(coord_value):\n",
    "        return pd.NA\n",
    "    \n",
    "    # If already numeric, return as is\n",
    "    if pd.api.types.is_numeric_dtype(type(coord_value)):\n",
    "        return float(coord_value)\n",
    "    \n",
    "    # Handle coordinate in string values\n",
    "    if isinstance(coord_value, str):\n",
    "        coord_string = coord_value.strip()\n",
    "\n",
    "        # Case 1: Check if it's a simple decimal with apostrophes and try simple conversion\n",
    "        cleaned = coord_string.replace(\"'\", \"\")\n",
    "        cleaned = cleaned.replace(\",\", \".\")\n",
    "        \n",
    "        try:\n",
    "            return float(cleaned)\n",
    "        except ValueError:\n",
    "            pass\n",
    "        \n",
    "        # Case 2: Parse DMS format like \"107°18'40.74\"E\" or \"6°17'23.45\"S\"\n",
    "        try:\n",
    "            degrees, minutes, seconds = 0, 0, 0\n",
    "            is_negative = False\n",
    "            \n",
    "            if coord_string.upper().endswith('S') or coord_string.upper().endswith('W'):\n",
    "                is_negative = True\n",
    "            \n",
    "            clean_str = coord_string.upper().replace('N', '').replace('S', '').replace('E', '').replace('W', '')\n",
    "            clean_str = clean_str.replace('\"', '').replace(\"'\", \"'\")\n",
    "            \n",
    "            # Parse components\n",
    "            if '°' in clean_str:\n",
    "                parts = clean_str.split('°')\n",
    "                degrees = float(parts[0])\n",
    "                if len(parts) > 1:\n",
    "                    if \"'\" in parts[1]:\n",
    "                        min_sec = parts[1].split(\"'\")\n",
    "                        minutes = float(min_sec[0])\n",
    "                        if len(min_sec) > 1 and min_sec[1]:\n",
    "                            seconds = float(min_sec[1])\n",
    "            \n",
    "            # Convert to decimal degrees\n",
    "            decimal_degrees = degrees + (minutes / 60) + (seconds / 3600)\n",
    "            \n",
    "            # Apply the negative sign if needed\n",
    "            if is_negative:\n",
    "                decimal_degrees = -decimal_degrees\n",
    "                \n",
    "            return decimal_degrees\n",
    "            \n",
    "        except (ValueError, IndexError, TypeError):\n",
    "            return pd.NA\n",
    "    \n",
    "    # For any other types, try direct conversion\n",
    "    try:\n",
    "        return float(coord_value)\n",
    "    except (ValueError, TypeError):\n",
    "        return pd.NA\n",
    "\n",
    "def process_coordinates(df, lat_col, lon_col, sheet_name=None, excel_name=None): #Process coordinates and error coordinates\n",
    "    if lat_col is None or lon_col is None:\n",
    "        print(f\"⚠️ Cannot process coordinates in '{sheet_name}': Missing coordinate column(s)\")\n",
    "        return df, []\n",
    "        \n",
    "    df_copy = df.copy()\n",
    "    error_rows = []\n",
    "    \n",
    "    # Process each row with better error tracking\n",
    "    for idx, row in df_copy.iterrows():\n",
    "        try:\n",
    "            # Defensive check to make sure the columns exist in the row\n",
    "            if lat_col not in row or lon_col not in row:\n",
    "                print(f\"⚠️ Row {idx} missing coordinate column(s) in '{sheet_name}'\")\n",
    "                continue\n",
    "                \n",
    "            lat_val = parse_coordinate(row[lat_col])\n",
    "            lon_val = parse_coordinate(row[lon_col])\n",
    "            \n",
    "            # Error tracking\n",
    "            if pd.isna(lat_val) and not pd.isna(row[lat_col]):\n",
    "                error_info = {\n",
    "                    'Excel File': excel_name,\n",
    "                    'Sheet': sheet_name,\n",
    "                    'Row Index': idx,\n",
    "                    'Original Lat Value': row[lat_col],\n",
    "                    'Original Lon Value': row[lon_col],\n",
    "                    'Error': f\"Failed to parse latitude: {row[lat_col]}\"\n",
    "                }\n",
    "                error_rows.append(error_info)\n",
    "            \n",
    "            if pd.isna(lon_val) and not pd.isna(row[lon_col]):\n",
    "                error_info = {\n",
    "                    'Excel File': excel_name,\n",
    "                    'Sheet': sheet_name,\n",
    "                    'Row Index': idx,\n",
    "                    'Original Lat Value': row[lat_col],\n",
    "                    'Original Lon Value': row[lon_col],\n",
    "                    'Error': f\"Failed to parse longitude: {row[lon_col]}\"\n",
    "                }\n",
    "                error_rows.append(error_info)\n",
    "            \n",
    "            df_copy.at[idx, lat_col] = lat_val\n",
    "            df_copy.at[idx, lon_col] = lon_val\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                'Excel File': excel_name,\n",
    "                'Sheet': sheet_name,\n",
    "                'Row Index': idx,\n",
    "                'Original Lat Value': str(row.get(lat_col, 'Column not found')),\n",
    "                'Original Lon Value': str(row.get(lon_col, 'Column not found')),\n",
    "                'Error': str(e)\n",
    "            }\n",
    "            error_rows.append(error_info)\n",
    "    \n",
    "    # Apply fix_coordinates with better error handling\n",
    "    try:\n",
    "        fixed_coords = df_copy.apply(fix_coordinates, axis=1, lat_col=lat_col, lon_col=lon_col)\n",
    "        df_copy[lat_col] = fixed_coords[0]\n",
    "        df_copy[lon_col] = fixed_coords[1]\n",
    "    except Exception as e:\n",
    "        error_info = {\n",
    "            'Excel File': excel_name,\n",
    "            'Sheet': sheet_name,\n",
    "            'Row Index': 'Multiple',\n",
    "            'Original Lat Value': 'Multiple',\n",
    "            'Original Lon Value': 'Multiple',\n",
    "            'Error': f\"Batch coordinate fixing failed: {str(e)}\"\n",
    "        }\n",
    "        error_rows.append(error_info)\n",
    "    \n",
    "    return df_copy, error_rows\n",
    "\n",
    "def sanitize_for_path(text):\n",
    "    if text is None:\n",
    "        return \"unknown\"\n",
    "    # Replace characters that are not safe for file paths\n",
    "    unsafe_chars = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|']\n",
    "    result = str(text)\n",
    "    for char in unsafe_chars:\n",
    "        result = result.replace(char, '_')\n",
    "    return result.strip()\n",
    "\n",
    "def add_image_documentation_paths(gdf, excel_name, sheet_name, output_base_dir):\n",
    "    # Find documentation column\n",
    "    doc_columns = [col for col in gdf.columns if 'dokumentasi' in str(col).lower()]\n",
    "    \n",
    "    if not doc_columns:\n",
    "        # If no dokumentasi column exists, add a placeholder one\n",
    "        gdf['Image Dokumentasi'] = None\n",
    "        doc_columns = ['Image Dokumentasi']\n",
    "    \n",
    "    # Use the first found dokumentasi column\n",
    "    doc_column = doc_columns[0]\n",
    "    \n",
    "    # Clean names for use in paths\n",
    "    file_name_clean = sanitize_for_path(excel_name)\n",
    "    safe_sheet_name = sanitize_for_path(sheet_name)\n",
    "    safe_column_name = sanitize_for_path(doc_column)\n",
    "    \n",
    "    # Generate image paths for each row\n",
    "    for idx, row in gdf.iterrows():\n",
    "        safe_row_identifier = sanitize_for_path(idx)\n",
    "        \n",
    "        # Get Kota/Kabupaten name, if available\n",
    "        kota_kab = \"Unknown\"\n",
    "        if 'Kota/Kabupaten' in gdf.columns and pd.notna(row['Kota/Kabupaten']):\n",
    "            kota_kab = sanitize_for_path(row['Kota/Kabupaten'])\n",
    "        \n",
    "        # Construct the image path with the requested structure\n",
    "        image_path = os.path.join(\n",
    "            output_base_dir,\n",
    "            \"Extract Images\",\n",
    "            \"Dokumentasi\",\n",
    "            f\"{file_name_clean}_Sheet_{safe_sheet_name}_Column_{safe_column_name}_Row{safe_row_identifier}.png\"\n",
    "        )\n",
    "        \n",
    "        # Assign to the dataframe\n",
    "        gdf.at[idx, 'Image Dokumentasi'] = image_path\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def add_image_paths(gdf, excel_name, sheet_name, output_base_dir):\n",
    "    # Clean names for use in paths\n",
    "    file_name_clean = sanitize_for_path(excel_name)\n",
    "    safe_sheet_name = sanitize_for_path(sheet_name)\n",
    "    \n",
    "    # Check if it's a Rambu sheet\n",
    "    if 'rambu' in sheet_name.lower():\n",
    "        # Check if 'Nama Rambu' column exists\n",
    "        nama_rambu_columns = [col for col in gdf.columns if 'nama rambu' in str(col).lower()]\n",
    "        \n",
    "        if nama_rambu_columns:\n",
    "            nama_rambu_column = nama_rambu_columns[0]\n",
    "            \n",
    "            # Generate image paths for each row\n",
    "            for idx, row in gdf.iterrows():\n",
    "                nama_rambu_value = str(row.get(nama_rambu_column, ''))\n",
    "                \n",
    "                if nama_rambu_value and nama_rambu_value.lower() != 'nan' and nama_rambu_value.lower() != 'none':\n",
    "                    # Construct the image path using the Nama Rambu value\n",
    "                    safe_nama_rambu = sanitize_for_path(nama_rambu_value)\n",
    "                    image_path = os.path.join(output_base_dir, \"Extract Images\", \"Rambu\", f\"{safe_nama_rambu}.png\")\n",
    "                    \n",
    "                    # Assign the constructed path to the 'Image Rambu' property\n",
    "                    gdf.at[idx, 'Image Rambu'] = image_path\n",
    "                else:\n",
    "                    gdf.at[idx, 'Image Rambu'] = None  # Handle missing Nama Rambu values\n",
    "    \n",
    "    # Check if it's an RPPJ sheet\n",
    "    elif 'rppj' in sheet_name.lower():\n",
    "        # Generate image paths for each row\n",
    "        for idx, row in gdf.iterrows():\n",
    "            safe_row_identifier = sanitize_for_path(idx)\n",
    "            \n",
    "            # Construct the image path for RPPJ\n",
    "            image_path = os.path.join(\n",
    "                output_base_dir,\n",
    "                \"Extract Images\",\n",
    "                \"RPPJ\",\n",
    "                f\"{file_name_clean}_Sheet_{safe_sheet_name}_Column_RPPJ_Row{safe_row_identifier}.png\"\n",
    "            )\n",
    "            \n",
    "            # Assign to the dataframe\n",
    "            gdf.at[idx, 'Image RPPJ'] = image_path\n",
    "    \n",
    "    return gdf\n",
    "\n",
    "def save_to_geojson(gdf, output_path, batas_wilayah=None, excel_name=None, sheet_name=None, output_base_dir=None):\n",
    "    try:\n",
    "        gdf = gdf.copy()\n",
    "        \n",
    "        # Make sure we have a valid geometry column\n",
    "        if 'geometry' not in gdf.columns and 'Geometry' not in gdf.columns:\n",
    "            print(f\"❌ Error: No geometry column found in data for {os.path.basename(output_path)}\")\n",
    "            return\n",
    "            \n",
    "        # Ensure the GeoDataFrame has a proper geometry column set\n",
    "        if not isinstance(gdf, gpd.GeoDataFrame):\n",
    "            if 'geometry' in gdf.columns:\n",
    "                gdf = gpd.GeoDataFrame(gdf, geometry='geometry', crs=\"EPSG:4326\")\n",
    "            elif 'Geometry' in gdf.columns:\n",
    "                gdf = gpd.GeoDataFrame(gdf, geometry='Geometry', crs=\"EPSG:4326\")\n",
    "            else:\n",
    "                print(f\"❌ Error: Cannot create GeoDataFrame - no geometry column found\")\n",
    "                return\n",
    "        else:\n",
    "            # Explicitly set the geometry column even if it's already a GeoDataFrame\n",
    "            if 'geometry' in gdf.columns:\n",
    "                gdf = gdf.set_geometry('geometry')\n",
    "            elif 'Geometry' in gdf.columns:\n",
    "                gdf = gdf.set_geometry('Geometry')\n",
    "        \n",
    "        # Ensure the GeoDataFrame has a valid CRS\n",
    "        if gdf.crs is None:\n",
    "            gdf = gdf.set_crs(\"EPSG:4326\")\n",
    "        \n",
    "        # Perform spatial join with batas_wilayah if provided\n",
    "        if batas_wilayah is not None:\n",
    "            try:                \n",
    "                # Make sure we have valid geometries\n",
    "                gdf = gdf[~gdf.geometry.isna()].copy()\n",
    "                \n",
    "                # Make sure CRS matches for join\n",
    "                if gdf.crs != batas_wilayah.crs:\n",
    "                    gdf = gdf.to_crs(batas_wilayah.crs)\n",
    "                \n",
    "                # Perform the spatial join\n",
    "                gdf = gpd.sjoin(gdf, batas_wilayah[['geometry', 'NAMOBJ']], how=\"left\", predicate=\"intersects\")\n",
    "                \n",
    "                # Rename NAMOBJ column to Kota/Kabupaten\n",
    "                if 'NAMOBJ' in gdf.columns:\n",
    "                    gdf = gdf.rename(columns={'NAMOBJ': 'Kota/Kabupaten'})\n",
    "                \n",
    "                # Clean up index column created by spatial join\n",
    "                if 'index_right' in gdf.columns:\n",
    "                    gdf = gdf.drop(columns=['index_right'])\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error during spatial join: {str(e)}\")\n",
    "        \n",
    "        # Add the image documentation paths before saving\n",
    "        if excel_name is not None and sheet_name is not None and output_base_dir is not None:\n",
    "            # Add documentation image paths\n",
    "            gdf = add_image_documentation_paths(gdf, excel_name, sheet_name, output_base_dir)\n",
    "            \n",
    "            # Add new special image paths based on sheet type\n",
    "            gdf = add_image_paths(gdf, excel_name, sheet_name, output_base_dir)\n",
    "        \n",
    "        # Modify the output path to include Kota/Kabupaten and \"Jalan Eksisting\"\n",
    "        if 'Kota/Kabupaten' in gdf.columns:\n",
    "            # Group by Kota/Kabupaten and save each group to the appropriate directory\n",
    "            for name_obj, group in gdf.groupby('Kota/Kabupaten'):\n",
    "                if pd.isna(name_obj):\n",
    "                    name_obj = \"Unknown\"\n",
    "                    \n",
    "                # Create directory structure: output_folder/Extract GeoJSON/Kota/Kabupaten/Jalan Eksisting\n",
    "                output_dir = os.path.dirname(output_path)\n",
    "                file_name = os.path.basename(output_path)\n",
    "                \n",
    "                # Create new path with Kota/Kabupaten and Jalan Eksisting folders\n",
    "                new_output_dir = os.path.join(output_dir, name_obj, \"Jalan Eksisting\")\n",
    "                os.makedirs(new_output_dir, exist_ok=True)\n",
    "                \n",
    "                new_output_path = os.path.join(new_output_dir, file_name)\n",
    "                \n",
    "                # Save the GeoJSON file\n",
    "                clean_geojson(group, new_output_path)\n",
    "        else:\n",
    "            # If Kota/Kabupaten is not in columns, just save to the original path\n",
    "            clean_geojson(gdf, output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error saving GeoJSON {output_path}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "def clean_geojson(gdf, output_path):  # Save GeoDataFrame in a clean format GeoJSON file\n",
    "    temp_path = output_path.replace(\".geojson\", \"_temp.geojson\")\n",
    "    gdf.to_file(temp_path, driver=\"GeoJSON\")\n",
    "    \n",
    "    with open(temp_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        geojson_data = json.load(file)\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(geojson_data, file, indent=4)\n",
    "    \n",
    "    os.remove(temp_path)\n",
    "    print(f\"✅ Saved: {output_path}\")\n",
    "\n",
    "def flatten_excel_to_geojson(file_path, output_folder, excel_name=None, batas_wilayah=None, error_logs=None):\n",
    "    if error_logs is None:\n",
    "        error_logs = []\n",
    "    \n",
    "    try:\n",
    "        # If excel_name is not provided, extract it from the file_path\n",
    "        if excel_name is None:\n",
    "            excel_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        \n",
    "        # Create output folder if it doesn't exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        \n",
    "        # Extract the output base directory from output_folder\n",
    "        output_base_dir = os.path.dirname(os.path.dirname(output_folder))\n",
    "        \n",
    "        # Load workbook\n",
    "        wb = load_workbook(file_path, data_only=True)\n",
    "\n",
    "        # Process each sheet\n",
    "        for sheet_name in wb.sheetnames:\n",
    "            try:\n",
    "                ws = wb[sheet_name]\n",
    "\n",
    "                # Unmerge cells and fill values\n",
    "                for merge in list(ws.merged_cells):\n",
    "                    ws.unmerge_cells(str(merge))\n",
    "                    top_left = ws.cell(merge.min_row, merge.min_col).value\n",
    "                    for row in range(merge.min_row, merge.max_row + 1):\n",
    "                        for col in range(merge.min_col, merge.max_col + 1):\n",
    "                            ws.cell(row, col, top_left)\n",
    "\n",
    "                # Convert to DataFrame\n",
    "                data = list(ws.values)\n",
    "                df = pd.DataFrame(data)\n",
    "\n",
    "                # Check if DataFrame is empty or has too few rows\n",
    "                if df.empty or len(df) < 3:\n",
    "                    print(f\"⚠️ Skipping '{sheet_name}' (Empty sheet or insufficient data)\")\n",
    "                    continue\n",
    "\n",
    "                # Find rows containing \"NO\" (safer approach)\n",
    "                try:\n",
    "                    header_indices = df[df.apply(lambda x: x.astype(str).str.contains(\"NO\", case=False, na=False)).any(axis=1)].index\n",
    "                    if len(header_indices) == 0:\n",
    "                        print(f\"⚠️ Skipping '{sheet_name}' (No header row with 'NO' found)\")\n",
    "                        continue\n",
    "                    header_index = header_indices[0]\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Skipping '{sheet_name}' (Error finding header row: {str(e)})\")\n",
    "                    continue\n",
    "\n",
    "                # Use the identified header row\n",
    "                df.columns = df.iloc[header_index].astype(str).str.strip()\n",
    "\n",
    "                # Remove empty columns\n",
    "                df = df.dropna(axis=1, how=\"all\")\n",
    "\n",
    "                # First pass of REKAP filtering\n",
    "                try:\n",
    "                    rekap_mask = df.columns.astype(str).str.contains(\"REKAP\", case=False, na=False)\n",
    "                    if rekap_mask.any():\n",
    "                        df = df.loc[:, ~rekap_mask]\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning in '{sheet_name}': Error filtering REKAP columns - {str(e)}\")\n",
    "\n",
    "                # Drop rows above and including header, plus the empty row after header\n",
    "                rows_to_drop = list(range(0, header_index + 2))\n",
    "                if len(df) > max(rows_to_drop) + 1:  # Make sure we have enough rows\n",
    "                    df = df.drop(index=rows_to_drop).reset_index(drop=True)\n",
    "                else:\n",
    "                    print(f\"⚠️ Skipping '{sheet_name}' (Not enough data rows after header)\")\n",
    "                    continue\n",
    "\n",
    "                # Improved header row merging\n",
    "                if len(df) >= 2:\n",
    "                    first_row = df.iloc[0].astype(str).replace('None', '').replace('nan', '')\n",
    "                    second_row = df.iloc[1].astype(str).replace('None', '').replace('nan', '')\n",
    "                    \n",
    "                    # Smart merging to avoid duplication\n",
    "                    merged_header = []\n",
    "                    for a, b in zip(first_row, second_row):\n",
    "                        a = a.strip()\n",
    "                        b = b.strip()\n",
    "                        \n",
    "                        # Skip columns with \"REKAP\" in the name\n",
    "                        if \"rekap\" in a.lower() or \"rekap\" in b.lower():\n",
    "                            merged_header.append(\"TO_BE_REMOVED\")  # Mark for removal\n",
    "                            continue\n",
    "                        \n",
    "                        if not a and not b:\n",
    "                            merged_header.append(\"Column_\" + str(len(merged_header)))\n",
    "                        elif not a:\n",
    "                            merged_header.append(b)\n",
    "                        elif not b:\n",
    "                            merged_header.append(a)\n",
    "                        else:\n",
    "                            if a.lower() in b.lower():\n",
    "                                merged_header.append(b)\n",
    "                            elif b.lower() in a.lower():\n",
    "                                merged_header.append(a)\n",
    "                            else:\n",
    "                                merged_header.append(f\"{a} {b}\")\n",
    "                    \n",
    "                    # Ensure column names are unique\n",
    "                    df.columns = unique_column_names(merged_header)\n",
    "                    \n",
    "                    # Remove any columns marked for removal\n",
    "                    df = df.loc[:, ~df.columns.str.contains(\"TO_BE_REMOVED\")]\n",
    "                    \n",
    "                    # Remove the first two rows used for headers\n",
    "                    df = df.drop(index=[0, 1]).reset_index(drop=True)\n",
    "                else:\n",
    "                    print(f\"⚠️ Skipping '{sheet_name}' (Not enough rows for headers)\")\n",
    "                    continue\n",
    "\n",
    "                # Second pass of REKAP filtering after merging headers\n",
    "                try:\n",
    "                    rekap_mask = df.columns.astype(str).str.contains(\"REKAP\", case=False, na=False)\n",
    "                    if rekap_mask.any():\n",
    "                        df = df.loc[:, ~rekap_mask]\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning in '{sheet_name}': Error filtering REKAP columns (second pass) - {str(e)}\")\n",
    "\n",
    "                # Normalize column names for consistent detection\n",
    "                df.columns = [str(col).lower().strip() if col is not None else f\"col_{i}\" for i, col in enumerate(df.columns)]\n",
    "\n",
    "                # Third pass of REKAP filtering after normalizing\n",
    "                try:\n",
    "                    rekap_mask = df.columns.str.contains(\"rekap\", case=False, na=False)\n",
    "                    if rekap_mask.any():\n",
    "                        df = df.loc[:, ~rekap_mask]\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning in '{sheet_name}': Error filtering rekap columns (third pass) - {str(e)}\")\n",
    "\n",
    "                # Check if this sheet is about MARKA or PAGAR PENGAMAN\n",
    "                is_marka_sheet = \"marka\" in sheet_name.lower() or any(col for col in df.columns if isinstance(col, str) and \"marka\" in col.lower())\n",
    "                is_pagar_pengaman_sheet = \"pagar pengaman\" in sheet_name.lower() or any(col for col in df.columns if isinstance(col, str) and \"pagar pengaman\" in col.lower())\n",
    "\n",
    "                # Find all coordinate columns using the improved function\n",
    "                start_lat_col = find_coordinate_columns(df, 'start', 'lat')\n",
    "                start_lon_col = find_coordinate_columns(df, 'start', 'lon')\n",
    "                end_lat_col = find_coordinate_columns(df, 'end', 'lat')\n",
    "                end_lon_col = find_coordinate_columns(df, 'end', 'lon')\n",
    "\n",
    "                # Check available coordinate patterns\n",
    "                has_start_coords = start_lat_col is not None and start_lon_col is not None\n",
    "                has_end_coords = end_lat_col is not None and end_lon_col is not None\n",
    "\n",
    "                if not has_start_coords:\n",
    "                    print(f\"⚠️ Skipping '{sheet_name}' (No valid start coordinate columns found)\")\n",
    "                    continue\n",
    "                \n",
    "                # Filter rows where both latitude and longitude values are blank\n",
    "                if has_start_coords:\n",
    "                    # Convert coordinates to appropriate types and handle formatting issues\n",
    "                    coord_mask = df[start_lat_col].astype(str).str.strip().replace('', np.nan).notna() & \\\n",
    "                                df[start_lon_col].astype(str).str.strip().replace('', np.nan).notna()\n",
    "                    \n",
    "                    if coord_mask.any():\n",
    "                        # Only process rows with actual coordinate data\n",
    "                        df_with_coords = df[coord_mask].reset_index(drop=True)\n",
    "                        df_processed, start_errors = process_coordinates(df_with_coords, start_lat_col, start_lon_col, sheet_name, excel_name)\n",
    "                        \n",
    "                        # Update only the rows that had coordinates\n",
    "                        df = df.copy()\n",
    "                        df.loc[coord_mask, :] = df_processed\n",
    "                        \n",
    "                        error_logs.extend(start_errors)\n",
    "                    else:\n",
    "                        print(f\"⚠️ No valid start coordinates found in '{sheet_name}'\")\n",
    "                        continue\n",
    "\n",
    "                if has_end_coords:\n",
    "                    # Similar filtering for end coordinates\n",
    "                    coord_mask = df[end_lat_col].astype(str).str.strip().replace('', np.nan).notna() & \\\n",
    "                                df[end_lon_col].astype(str).str.strip().replace('', np.nan).notna()\n",
    "                    \n",
    "                    if coord_mask.any():\n",
    "                        df_with_coords = df[coord_mask].reset_index(drop=True)\n",
    "                        df_processed, end_errors = process_coordinates(df_with_coords, end_lat_col, end_lon_col, sheet_name, excel_name)\n",
    "                        \n",
    "                        df.loc[coord_mask, :] = df_processed\n",
    "                        error_logs.extend(end_errors)\n",
    "\n",
    "                # Check if the end coordinates actually contain valid data\n",
    "                if has_end_coords:\n",
    "                    has_valid_end_coords = not df[end_lat_col].isna().all() and not df[end_lon_col].isna().all()\n",
    "                    valid_pairs = ((df[start_lat_col].notna() & df[start_lon_col].notna()) & (df[end_lat_col].notna() & df[end_lon_col].notna())).any()\n",
    "                else:\n",
    "                    has_valid_end_coords = False\n",
    "                    valid_pairs = False\n",
    "                \n",
    "                # Determine geometry type based on available coordinates, actual data, and sheet type\n",
    "                # MARKA sheets should use MultiPoint geometry\n",
    "                if is_marka_sheet:\n",
    "                    print(f\"Processing '{sheet_name}' as MultiPoint (MARKA sheet)\")\n",
    "\n",
    "                    # First check if we have any valid coordinates before applying\n",
    "                    has_valid_coords = ((df[start_lat_col].notna() & df[start_lon_col].notna()) | \n",
    "                                        (has_end_coords and df[end_lat_col].notna() & df[end_lon_col].notna())).any()\n",
    "                    \n",
    "                    if not has_valid_coords:\n",
    "                        print(f\"⚠️ Skipping '{sheet_name}' (No valid coordinates found in MARKA sheet)\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Create MultiPoint geometry only for rows with valid coordinates\n",
    "                    def create_multipoint(row):\n",
    "                        try:\n",
    "                            points = []\n",
    "                            # Add start point if valid\n",
    "                            if pd.notna(row[start_lon_col]) and pd.notna(row[start_lat_col]):\n",
    "                                points.append((float(row[start_lon_col]), float(row[start_lat_col])))\n",
    "                                \n",
    "                            # Add end point if valid and available\n",
    "                            if has_end_coords and pd.notna(row[end_lon_col]) and pd.notna(row[end_lat_col]):\n",
    "                                points.append((float(row[end_lon_col]), float(row[end_lat_col])))\n",
    "                                \n",
    "                            # Return MultiPoint if we have points, else None\n",
    "                            return MultiPoint(points) if points else None\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating MultiPoint for row {row.name}: {str(e)}\")\n",
    "                            return None\n",
    "                    \n",
    "                    # Apply the function to create geometry\n",
    "                    df[\"geometry\"] = df.apply(create_multipoint, axis=1)\n",
    "                    \n",
    "                    # Exclude coordinate columns from properties\n",
    "                    exclude_cols = [start_lat_col, start_lon_col]\n",
    "                    if has_end_coords:\n",
    "                        exclude_cols.extend([end_lat_col, end_lon_col])\n",
    "                    exclude_cols.append(\"geometry\")\n",
    "                    \n",
    "                # PAGAR PENGAMAN sheets with valid start/end coordinates should use LineString\n",
    "                elif is_pagar_pengaman_sheet and has_valid_end_coords and valid_pairs:\n",
    "                    print(f\"Processing '{sheet_name}' as LineString (PAGAR PENGAMAN sheet)\")\n",
    "                    \n",
    "                    # Create LineString geometry with additional debugging and more flexible validation\n",
    "                    def create_linestring(row):\n",
    "                        try:\n",
    "                            # Enhanced debugging\n",
    "                            start_lat = row[start_lat_col]\n",
    "                            start_lon = row[start_lon_col]\n",
    "                            end_lat = row[end_lat_col]\n",
    "                            end_lon = row[end_lon_col]\n",
    "                            \n",
    "                            # Skip processing if all coordinates are blank/NaN\n",
    "                            if pd.isna(start_lat) and pd.isna(start_lon) and pd.isna(end_lat) and pd.isna(end_lon):\n",
    "                                return None\n",
    "                                \n",
    "                            # First try to parse any string coordinates\n",
    "                            if isinstance(start_lat, str):\n",
    "                                start_lat = parse_coordinate(start_lat)\n",
    "                            if isinstance(start_lon, str):\n",
    "                                start_lon = parse_coordinate(start_lon)\n",
    "                            if isinstance(end_lat, str):\n",
    "                                end_lat = parse_coordinate(end_lat)\n",
    "                            if isinstance(end_lon, str):\n",
    "                                end_lon = parse_coordinate(end_lon)\n",
    "                            \n",
    "                            # Create LineString if we have valid coordinates\n",
    "                            if (pd.notna(start_lon) and pd.notna(start_lat) and \n",
    "                                pd.notna(end_lon) and pd.notna(end_lat)):\n",
    "                                return LineString([\n",
    "                                    (float(start_lon), float(start_lat)),\n",
    "                                    (float(end_lon), float(end_lat))\n",
    "                                ])\n",
    "                            else:\n",
    "                                # If one set of coordinates is missing, fall back to Point\n",
    "                                if pd.notna(start_lon) and pd.notna(start_lat):\n",
    "                                    offset = 0.0001\n",
    "                                    return LineString([\n",
    "                                        (float(start_lon), float(start_lat)),\n",
    "                                        (float(start_lon) + offset, float(start_lat) + offset)\n",
    "                                    ])\n",
    "                                return None\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error creating LineString for row {row.name}: {str(e)}\")\n",
    "                            return None\n",
    "                    \n",
    "                    df[\"geometry\"] = df.apply(create_linestring, axis=1)\n",
    "                    exclude_cols = [start_lat_col, start_lon_col, end_lat_col, end_lon_col, \"geometry\"]\n",
    "                    \n",
    "                else:\n",
    "                    # Other sheets use Point geometry (only start coordinates)\n",
    "                    print(f\"Processing '{sheet_name}' as Point geometry (regular sheet)\")\n",
    "                    \n",
    "                    # Create Point geometry with start coordinates, only for rows with valid data\n",
    "                    df[\"geometry\"] = df.apply(\n",
    "                        lambda row: Point(row[start_lon_col], row[start_lat_col]) \n",
    "                        if pd.notna(row[start_lon_col]) and pd.notna(row[start_lat_col]) else None,\n",
    "                        axis=1\n",
    "                    )\n",
    "                    \n",
    "                    # Exclude coordinate columns from properties\n",
    "                    exclude_cols = [start_lat_col, start_lon_col]\n",
    "                    if has_end_coords:\n",
    "                        exclude_cols.extend([end_lat_col, end_lon_col])\n",
    "                    exclude_cols.append(\"geometry\")\n",
    "\n",
    "                # Drop rows where geometry is None\n",
    "                df = df.dropna(subset=[\"geometry\"]).reset_index(drop=True)\n",
    "\n",
    "                # Only create GeoDataFrame if there are valid geometries\n",
    "                if len(df) > 0 and not df[\"geometry\"].isnull().all():\n",
    "                    # Filter REKAP columns before creating properties\n",
    "                    properties_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "                    properties_cols = [col for col in properties_cols if 'rekap' not in str(col).lower()]\n",
    "                    \n",
    "                    # Create GeoDataFrame with explicit geometry setting\n",
    "                    gdf = gpd.GeoDataFrame(df[properties_cols + [\"geometry\"]], geometry=\"geometry\",crs=\"EPSG:4326\")\n",
    "                    \n",
    "                    # Verify that the geometry column is properly set\n",
    "                    if not gdf.geometry.name == \"geometry\":\n",
    "                        gdf = gdf.set_geometry(\"geometry\")\n",
    "                    \n",
    "                    gdf.columns = clean_column_names(gdf.columns)\n",
    "\n",
    "                    gdf = gdf.loc[:, ~gdf.columns.astype(str).str.contains(\"rekap\", case=False, na=False)]\n",
    "                    \n",
    "                    gdf = gdf.loc[:, ~gdf.columns.astype(str).str.match(r\"^None$|None_\", na=False)]\n",
    "\n",
    "                    gdf.columns = gdf.columns.astype(str).str.replace(r\"\\sNone\\b\", \"\", regex=True).str.strip()\n",
    "\n",
    "                    # One final check for REKAP columns before saving\n",
    "                    if any('rekap' in str(col).lower() for col in gdf.columns):\n",
    "                        gdf = gdf.loc[:, ~gdf.columns.astype(str).str.contains(\"rekap\", case=False, na=False)]\n",
    "                    \n",
    "                    # Define output file path\n",
    "                    output_path = os.path.join(output_folder, f\"{excel_name}_{sheet_name}.geojson\")\n",
    "                    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "                    # Save as GeoJSON with additional parameters for image documentation\n",
    "                    save_to_geojson(\n",
    "                        gdf, \n",
    "                        output_path, \n",
    "                        batas_wilayah,\n",
    "                        excel_name=excel_name,\n",
    "                        sheet_name=sheet_name,\n",
    "                        output_base_dir=output_base_dir\n",
    "                    )\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error processing sheet '{sheet_name}': {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "\n",
    "        return error_logs\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing file: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return error_logs\n",
    "\n",
    "def process_single_excel_file(file_path, output_base_folder, batas_wilayah_path=None): # Process a single Excel file and convert it to GeoJSON\n",
    "    # Create output folder\n",
    "    output_folder = os.path.join(output_base_folder, \"Extract GeoJSON\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Load the city boundaries shapefile if provided\n",
    "    batas_wilayah = None\n",
    "    if batas_wilayah_path and os.path.exists(batas_wilayah_path):\n",
    "        try:\n",
    "            batas_wilayah = gpd.read_file(batas_wilayah_path)\n",
    "            print(f\"✅ Loaded city boundaries from: {batas_wilayah_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading city boundaries shapefile: {str(e)}\")\n",
    "    else:\n",
    "        print(\"❌ No city boundaries provided or file not found.\")\n",
    "    \n",
    "    # Get file name\n",
    "    file_name = os.path.basename(file_path)\n",
    "    excel_name = os.path.splitext(file_name)[0]\n",
    "\n",
    "    error_logs = []\n",
    "    try:\n",
    "        # Process the file\n",
    "        error_logs = flatten_excel_to_geojson(file_path, output_folder, excel_name, batas_wilayah, error_logs)\n",
    "        print(f\"✅ Completed processing: {file_name}\")\n",
    "        if error_logs:\n",
    "            print(f\"⚠️ Found {len(error_logs)} coordinate errors during processing\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {file_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(f\"\\nProcessing: {file_name}\")\n",
    "    print(f\"\\n🎉 Excel file processed. Output saved to: {output_folder}\")\n",
    "\n",
    "def process_excel_folder_geojson(input_folder, output_base_folder, batas_wilayah_path=None): # Process a folder contain excel files and convert it to GeoJSON\n",
    "    output_folder = os.path.join(output_base_folder, \"Extract GeoJSON\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Load the city boundaries shapefile if provided\n",
    "    batas_wilayah = None\n",
    "    if batas_wilayah_path and os.path.exists(batas_wilayah_path):\n",
    "        try:\n",
    "            batas_wilayah = gpd.read_file(batas_wilayah_path)\n",
    "            print(f\"✅ Loaded city boundaries from: {batas_wilayah_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading city boundaries shapefile: {str(e)}\")\n",
    "    else:\n",
    "        print(\"❌ No city boundaries provided or file not found.\")\n",
    "    \n",
    "    # Get all Excel files in the input folder\n",
    "    excel_extensions = ['*.xlsx', '*.xls', '*.xlsm']\n",
    "    excel_files = []\n",
    "    \n",
    "    for ext in excel_extensions:\n",
    "        excel_files.extend(glob.glob(os.path.join(input_folder, ext)))\n",
    "    \n",
    "    # Initialize error log list\n",
    "    all_error_logs = []\n",
    "    \n",
    "    # Process each Excel file\n",
    "    for i, file_path in enumerate(excel_files, 1):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        excel_name = os.path.splitext(file_name)[0]\n",
    "        print(f\"\\n[{i}/{len(excel_files)}] Processing: {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Collect errors from processing this file\n",
    "            file_errors = flatten_excel_to_geojson(file_path, output_folder, excel_name, batas_wilayah, [])\n",
    "            all_error_logs.extend(file_errors)\n",
    "            print(f\"✅ Completed processing: {file_name}\")\n",
    "            if file_errors:\n",
    "                print(f\"⚠️ Found {len(file_errors)} coordinate errors during processing\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {file_name}: {str(e)}\")\n",
    "        \n",
    "    print(f\"\\n🎉 All Excel files processed. Output saved to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Run Function Excel to GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded city boundaries from: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export\\- Batas wilayah kota\\Batas_Kota_Kabupaten_JABAR.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanzi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'RAMBU' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_RAMBU.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kota Bekasi\\Jalan Eksisting\\01. Cileungsi - Cibeet_RAMBU.geojson\n",
      "Processing 'PJU' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_PJU.geojson\n",
      "Processing 'RPPJ' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_RPPJ.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kota Bekasi\\Jalan Eksisting\\01. Cileungsi - Cibeet_RPPJ.geojson\n",
      "Processing 'PAGAR PENGAMAN' as LineString (PAGAR PENGAMAN sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_PAGAR PENGAMAN.geojson\n",
      "Processing 'MARKA' as MultiPoint (MARKA sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_MARKA.geojson\n",
      "Processing 'WARNING LIGHT' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_WARNING LIGHT.geojson\n",
      "Processing 'APILL' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_APILL.geojson\n",
      "Processing 'ZOSS' as Point geometry (regular sheet)\n",
      "Processing 'FAS PENYEBERANGAN' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_FAS PENYEBERANGAN.geojson\n",
      "⚠️ Skipping 'RAMBU PORTABLE' (No valid start coordinate columns found)\n",
      "⚠️ Skipping 'TRAFFIC CONE' (No valid start coordinate columns found)\n",
      "⚠️ Skipping 'WATER BARRIER' (No valid start coordinate columns found)\n",
      "Processing 'CERMIN TIKUNG' as Point geometry (regular sheet)\n",
      "✅ Completed processing: 01. Cileungsi - Cibeet.xlsx\n",
      "\n",
      "Processing: 01. Cileungsi - Cibeet.xlsx\n",
      "\n",
      "🎉 Excel file processed. Output saved to: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\n"
     ]
    }
   ],
   "source": [
    "input_folder = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Data Hasil Survey\\01. Cileungsi - Cibeet.xlsx\"  # Fill with the path file of excel\n",
    "output_base_folder = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\"  # Fill with the path folder of export result\n",
    "batas_wilayah = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export\\- Batas wilayah kota\\Batas_Kota_Kabupaten_JABAR.shp\"\n",
    "process_single_excel_file(input_folder, output_base_folder,batas_wilayah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded city boundaries from: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export\\- Batas wilayah kota\\Batas_Kota_Kabupaten_JABAR.shp\n",
      "\n",
      "[1/2] Processing: 01. Cileungsi - Cibeet.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanzi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'RAMBU' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_RAMBU.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kota Bekasi\\Jalan Eksisting\\01. Cileungsi - Cibeet_RAMBU.geojson\n",
      "Processing 'PJU' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_PJU.geojson\n",
      "Processing 'RPPJ' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_RPPJ.geojson\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kota Bekasi\\Jalan Eksisting\\01. Cileungsi - Cibeet_RPPJ.geojson\n",
      "Processing 'PAGAR PENGAMAN' as LineString (PAGAR PENGAMAN sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_PAGAR PENGAMAN.geojson\n",
      "Processing 'MARKA' as MultiPoint (MARKA sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_MARKA.geojson\n",
      "Processing 'WARNING LIGHT' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_WARNING LIGHT.geojson\n",
      "Processing 'APILL' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_APILL.geojson\n",
      "Processing 'ZOSS' as Point geometry (regular sheet)\n",
      "Processing 'FAS PENYEBERANGAN' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Bogor\\Jalan Eksisting\\01. Cileungsi - Cibeet_FAS PENYEBERANGAN.geojson\n",
      "⚠️ Skipping 'RAMBU PORTABLE' (No valid start coordinate columns found)\n",
      "⚠️ Skipping 'TRAFFIC CONE' (No valid start coordinate columns found)\n",
      "⚠️ Skipping 'WATER BARRIER' (No valid start coordinate columns found)\n",
      "Processing 'CERMIN TIKUNG' as Point geometry (regular sheet)\n",
      "✅ Completed processing: 01. Cileungsi - Cibeet.xlsx\n",
      "\n",
      "[2/2] Processing: Ludira 27.A Jl. Bhayangkara (Pelabuhan Ratu).xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanzi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Conditional Formatting extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'RAMBU' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Sukabumi\\Jalan Eksisting\\Ludira 27.A Jl. Bhayangkara (Pelabuhan Ratu)_RAMBU.geojson\n",
      "Processing 'PJU' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Sukabumi\\Jalan Eksisting\\Ludira 27.A Jl. Bhayangkara (Pelabuhan Ratu)_PJU.geojson\n",
      "Processing 'RPPJ' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Sukabumi\\Jalan Eksisting\\Ludira 27.A Jl. Bhayangkara (Pelabuhan Ratu)_RPPJ.geojson\n",
      "Processing 'PAGAR PENGAMAN' as LineString (PAGAR PENGAMAN sheet)\n",
      "Processing 'MARKA' as MultiPoint (MARKA sheet)\n",
      "Processing 'WARNING LIGHT' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Sukabumi\\Jalan Eksisting\\Ludira 27.A Jl. Bhayangkara (Pelabuhan Ratu)_WARNING LIGHT.geojson\n",
      "Processing 'APILL' as Point geometry (regular sheet)\n",
      "✅ Saved: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\\Kabupaten Sukabumi\\Jalan Eksisting\\Ludira 27.A Jl. Bhayangkara (Pelabuhan Ratu)_APILL.geojson\n",
      "Processing 'ZOSS' as Point geometry (regular sheet)\n",
      "Processing 'FAS PENYEBERANGAN' as Point geometry (regular sheet)\n",
      "⚠️ Skipping 'RAMBU PORTABLE' (No valid start coordinate columns found)\n",
      "⚠️ Skipping 'TRAFFIC CONE' (No valid start coordinate columns found)\n",
      "⚠️ Skipping 'WATER BARRIER' (No valid start coordinate columns found)\n",
      "Processing 'CERMIN TIKUNG' as Point geometry (regular sheet)\n",
      "✅ Completed processing: Ludira 27.A Jl. Bhayangkara (Pelabuhan Ratu).xlsx\n",
      "\n",
      "🎉 All Excel files processed. Output saved to: C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\\Extract GeoJSON\n"
     ]
    }
   ],
   "source": [
    "input_folder = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Data Hasil Survey\"  # Fill with the path file of excel\n",
    "output_base_folder = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export1\"  # Fill with the path folder of export result\n",
    "batas_wilayah = r\"C:\\Users\\kanzi\\Documents\\Part Time Job\\Hasil Export\\- Batas wilayah kota\\Batas_Kota_Kabupaten_JABAR.shp\"\n",
    "process_excel_folder_geojson(input_folder, output_base_folder,batas_wilayah) # Run the function!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
